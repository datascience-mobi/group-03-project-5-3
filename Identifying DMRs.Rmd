---
title: "DMR search 2"
author: "Leo Burmedi, Pierre de Marinis, Konstantin Fischer, Daniel Ürge"
date: "7/5/2019"
output: word_document

---

```{r, include=FALSE}
options(tinytex.verbose = TRUE)
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# X. Searching for DMRs in our Data Set

Having sorted through our genes for favorable traits, it is now time to turn to the search for DMRs. In order to perform this search we will require the ‘T’ data sets, the ‘Resource’ data sets and the list of sequences with favorable traits acquired in the PCA and Klustering analysis.

### X.1 Loading Relevant Data

First we load relevant packages for further analysis and give a reminder as to what the T and Resource data sets contain. The qvalue package can be installed from https://github.com/StoreyLab/qvalue.

```{r Data Sets and Packages} 

library(qvalue)
library(knitr)


load("~/A. Leo/0. University/4. Semester (2.2 + 4.1)/4.1 FS/Bioinformatik Projekt/z. Local Work/data/Data for Rmarkdown.RData")

kable(head(g_T, n = 4), align = "c") 
kable(head(g_Resource, n = 4), row.names = FALSE, align = "c")

```

As you can see, the Resource data set contains relevant infomration regarding the name of a sequence and the fold change from the Mono to the AML cohort. The T data set represents all the inputs required for a t-test.

### X.2 Performing T-Tests

In the following we perform two-sided Student t-tests to determine the significance of differential methylation between our cohorts. To do so we define a function that manually performs the t-test and another that applies it to our T datasets. The resulting p-values are also plotted in a histogramm to innspect if they are well behaved.

```{r T-tests} 

## (1) Defining and applying Functions 
    ## defining a general function that applies a student t-test based on given input statistics
    
    f_ttest_alt <- function(m1, m2, s1, s2, n1, n2) {
      if(m1 == m2) {
        p_value = 1
        
      } else {
        
        df = (n1 + n2 - 2)
        s12 = sqrt((((n1 - 1)*s1^2)+((n2 - 1)*s2^2))/df)
        
        t = ((m1 - m2)/(s12*sqrt((1/n1) + (1/n2))))
        
        p_value = 2*pt(-abs(t),df)
        
      }
    }


    ## defining two functions that will comb through T data sets and provide inputs for f_ttest_alt

    fg_ttest_apl <- function(x) {
      m1 <- g_T[x, 1]
      m2 <- g_T[x, 4]
      
      s1 <- g_T[x, 2]
      s2 <- g_T[x, 5]
      
      n1 <- g_T[x, 3]
      n2 <- g_T[x, 6]
      
      p_value <- f_ttest_alt(m1, m2, s1, s2, n1, n2)
      return(p_value)
    }
    
    fp_ttest_apl <- function(x) {
      m1 <- p_T[x, 1]
      m2 <- p_T[x, 4]
      
      s1 <- p_T[x, 2]
      s2 <- p_T[x, 5]
      
      n1 <- p_T[x, 3]
      n2 <- p_T[x, 6]
      
      p_value <- f_ttest_alt(m1, m2, s1, s2, n1, n2)
      return(p_value)
    }
    
    ## applying our application functions to g_t and p_t
    j <- data.frame(seq(1, nrow(g_T), 1))
    g_p_values <- apply(j, 1, fg_ttest_apl)
    
    j <- data.frame(seq(1, nrow(p_T), 1))
    p_p_values <- apply(j, 1, fp_ttest_apl)
    
    
## (2) Formatting our outputs
    ## We bind our new p_values to the T data sets
    g_T <- cbind(g_T, g_p_values)
    colnames(g_T) <- c(colnames(g_T)[1:(ncol(g_T)-1)], "p_values")
    
    p_T <- cbind(p_T, p_p_values)
    colnames(p_T) <- c(colnames(p_T)[1:(ncol(p_T)-1)], "p_values")
    
    ## removal of uneccesary data
    remove(g_p_values, fg_ttest_apl, f_ttest_alt, j)
    remove(p_p_values, fp_ttest_apl)

    
## (3) checking to make sure the p value distribution is well behaved
hist(g_T$p_values, 100, main = "Histogramm of P−Values for Genes", xlab="p−values",ylim = c(0, 13000))
hist(p_T$p_values, 100, main = "Histogramm of P-Values for Promoters", xlab = "p-values", ylim = c(0, 13000))

```

It is evident that both of these p-value distributions are well behaved, which is an important requirement for the next step in the analysis.

### X.3 Performing a Q-Value Correction

Since we just performed upwards of 80.000 t-tests it is important to correct for multiple testing. The q-value method is a similar approach to the Benjamini & Hochberg method of adressing this problem. 

This method is to not correct the false psoitive rate to the point that any false positive is unlikely, but instead refer to a more useful and descriptive metric, the false discovery rate (FDR). This gives the proportion of false positives amongst discoveries. Instead of avoiding false positives it guarantees them at a controllable and known rate.This approach has demonstrably more porwer than simply reducing the false positive rate. 

What differentiates q-values from the classic FDR is that they retain the same power regardless of the number of tests, have a higher power than the Benjamini Hochberg method and while having the same end point originate in two different mathematical approaches. An important note for the q-value method is that it assumes the p-values used are well behaved, and uses the right hand tail of the distribution to estimate what the FDR is at every p-value. Thus it is important not only that our p-values are properly distributed, but also that we apply the correction now, when the full distribution is present. To use this method at a later point in the analysis could lead to misleading conclusions. 

The q-value method was drawn from ~~Storey et al~~

```{r Q-Value Correction}

## First we order our data set so that it's order matches that of the q-value output
g_T <- g_T[order(g_T$p_values, decreasing = TRUE), ]
p_T <- p_T[order(p_T$p_values, decreasing = TRUE), ]
 
## Second, we apply the package loaded earlier to generate two objects that contain the 
## q-values we need
g_q <- qvalue(p = g_T$p_values)
p_q <- qvalue(p = p_T$p_values)

## Finally, we bind the q-values to their respective sequences for later reference. We do
## not remove insignificant sequences yet. in this process we generate new 'Tq' data sets
g_T_q <- cbind(g_T, g_q$qvalue)
colnames(g_T_q)[ncol(g_T_q)] <- c("q_values")

p_T_q <- cbind(p_T, p_q$qvalue)
colnames(p_T_q)[ncol(p_T_q)] <- c("q_values")

## A preview of the new Tq data sets
kable(head(g_T_q, n = 4), row.names = FALSE, align = "c")

## Removing uneccesary data sets
remove(g_q, p_q)

```

We now have q-values associated to every p-value for every sequence. The q-value associated with a p-value indicates the expected FDR if all p-values less than or equal to the one in question were seen as significant.For example, if we set our q-value threshold at 0.50 all rows with a p-value of 1 or lower would be significant (i.e. all our values would be significant and approximately half woulf be false positives).

### X.4 Extracting Desirable Sequences with Favorable Traits

Having identified our desirable sequences in the PCA, we will now be generating two 'T_reduced' data sets that consist only of said sequences extracted from the Tq data sets.

```{r PCA extraction}

## Extracting sequences deemed favorable after PCA analysis from the Reduced data sets

  g_T_reduced <- g_T_q[which(rownames(g_T_q) %in% g_PC1_names), ]
  p_T_reduced <- p_T_q[which(rownames(p_T_q) %in% p_PC1_names), ]
  
  print(paste0("Reduced genes data set from ", nrow(g_T_q), " to ", nrow(g_T_reduced), " sequences"))
  print(paste0("Reduced promoters data set from ", nrow(p_T_q), " to ", nrow(p_T_reduced), " sequences"))  
  
## Removing uneccesary data sets
  remove(g_T_q, g_PC1_names)
  remove(p_T_q, p_PC1_names)
  

```

### X.5 Merging Data Sets

With the previous step complete we will now merge the Resource data sets with the Reduced data sets. This will allow us to select for fold change in the next step, as well as more easily identify research targets later, because the symbol of each gene will be associated with it in the data set. The resulting data sat wil be the beginnings of the Finale data sets. At this point we also remove the t-test inputs from the combined data set, as they are not relevant in the presence of p-values.

```{r Merging Data Sets}

## Merging data sets
  g_Finale <- merge(g_T_reduced, g_Resource, by = 0, all = FALSE)
  rownames(g_Finale) <- g_Finale$Row.names
  g_Finale <- g_Finale[, c(8, 9, 12, 11, 10)]
  
  p_Finale <- merge(p_T_reduced, p_Resource, by = 0, all = FALSE)
  rownames(p_Finale) <- p_Finale$Row.names
  p_Finale <- p_Finale[, c(8, 9, 12, 11, 10)]

## Removing uneccesary data sets from the environment
  remove(g_T_reduced, p_T_reduced)
  
## A preview of the new Finale data sets
kable(head(g_Finale, n = 4), row.names = FALSE, align = "c")
  
```

As you can see, these data sets willl be ideal for later research. They give mostly only relevant values, such as markers of significance, biological relevance, and references to research these sequences by.

### X.6 Selection for Biologically Relevant Sequences

We will now remove all sequences with a q-value above 5% and all those that do not show a minimum of a 3/4 or 4/3 ratio in their fold change. These are the sequences we define as Biologically significant. This means that all of the sequences remaining will be significant with a 5% chance of being false positives, as well as being substantially different in their average expression.

```{r Biological Relevance}

## Determining how many genes were originally in the Finale data sets
  nrow_gF <- nrow(g_Finale)
  nrow_pF <- nrow(p_Finale)

## Selection for high fold changes and significance
  g_Finale <- g_Finale[g_Finale$q_values <= 0.05, ]
  g_Finale <- g_Finale[g_Finale$Foldchange_Beta < log2(3/4) | 
                         g_Finale$Foldchange_Beta > log2(4/3), ]
  
  p_Finale <- p_Finale[p_Finale$q_values <= 0.05, ]
  p_Finale <- p_Finale[p_Finale$Foldchange_Beta < log2(3/4) | 
                         p_Finale$Foldchange_Beta > log2(4/3), ]

## Sorting the data set for p-values for demonstrative purposes
  g_Finale <- g_Finale[order(g_Finale$p_values, decreasing = TRUE), ]
  p_Finale <- p_Finale[order(p_Finale$p_values, decreasing = TRUE), ]
  
## Displaying the number of remainin Sequences
  print(paste0("Reduced genes data set from ",
               nrow_gF,
               " to ",
               nrow(g_Finale),
               " sequences, Of which ",
               ceiling(nrow(g_Finale)*0.05),
               " are expected to be false discoveries."))
  
  print(paste0("Reduced promoters data set from ",
               nrow_pF,
               " to ",
               nrow(p_Finale),
               " sequences, Of which ",
               ceiling(nrow(p_Finale)*0.05),
               " are expected to be false discoveries.")) 
  
## Preview of the filtered data sets
  kable(head(g_Finale, n = 1), row.names = FALSE, align = "c")
  kable(head(p_Finale, n = 1), row.names = FALSE, align = "c")
  
## Removing uneccesary data from the environment
  remove(nrow_gF, nrow_pF)

```

As the output indicates, the genes data frame shrinks significantly more than the promoters data set, which falls in line with the observation that the promoters data set has had clearer internal structures throughout the entire analysis. Both show a workable sum of of Biologically relevant genes. 

In addition, the previews of the ordered data sets show us that the last significant p-value ended up being between 3.3-3.9% in both cases. This would be equivalent to our mutliple testing corrected p-value.


### X.7 Plotting of Results

Having completed all steps necessary for the testing portion of this analysis, we will now create a volcano plot of all genes, marking the levels at which relevance/significance was cut. In order to do this we will be referencing the original T data sets after their merging with the Resource data sets (called the Finale Comparison data sets). Each Volcano plot will be for genes and promoters respectively, and the marked significance is equal to the largest p-value included in each respective Finale data set.

```{r Plotting Final Results}

## Final comparative results and plotting for genes
  g_FinaleComparison <- merge(g_T, g_Resource, by = 0, all = FALSE)
  rownames(g_FinaleComparison) <- g_FinaleComparison$Row.names
  g_FinaleComparison <- g_FinaleComparison[, c(8,11, 10, 9)]
  
  smoothScatter(g_FinaleComparison$Foldchange_Beta, -log10(g_FinaleComparison$p_values), 
                nbin = 1000, 
                colramp = colorRampPalette(c("white", "red")), 
                main = "Volcano Plot Genes", 
                ylab = "Significance", 
                xlab = "Fold-Change", 
                xlim = c(-20, 25), 
                ylim = c(0, 13))
  
  abline(h = -log10(max(g_Finale$p_values)), col = "blue")
  abline(v = log2(3/4), col = "blue")
  abline(v = log2(4/3), col = "blue")
  
## Final comparative results and plotting for promoters
  p_FinaleComparison <- merge(p_T, p_Resource, by = 0, all = FALSE)
  rownames(p_FinaleComparison) <- p_FinaleComparison$Row.names
  p_FinaleComparison <- p_FinaleComparison[, c(8,11, 10, 9)]
  
  smoothScatter(p_FinaleComparison$Foldchange_Beta, -log10(p_FinaleComparison$p_values), 
                nbin = 1000, 
                colramp = colorRampPalette(c("white", "red")), 
                main = "Volcano Plot Promoters", 
                ylab = "Significance", 
                xlab = "Fold-Change", 
                xlim = c(-20, 25), 
                ylim = c(0, 13))
  
  abline(h = -log10(max(p_Finale$p_values)), col = "blue")
  abline(v = log2(3/4), col = "blue")
  abline(v = log2(4/3), col = "blue")
  
## Removing uneccesary data sets
  
  remove(g_T, g_Resource, g_M_NA, g_FinaleComparison)
  remove(p_T, p_Resource, p_M_NA, p_FinaleComparison)

```

These outputs show us multiple trends. For one, the promoter results show a trend of higher significance and relevance than genes. Another point is that both genes and promoters show a distinct cloud of data that is significantly hypermethylated branching off of the central non-relevant area. In addition, both graphs show an area of less significant and relevant hypomethylations also taking place. Both graphs show a high density of points in non-significant and non-relevant areas of the graph, which indicates that a high proportion of sequences are not directly impacted in AML. Finally, both graphs show "arms" extending in the lower portion of the graph. These areas of mostly dubious significance and incredibly high Fold-Change likely indicate areas where small means with large standard deviations create this form of distortion. These arms are weaker on the promoter plot, once more in line with the higher quality of data we have come to expect from the promoters data set.







# Y. Logistic Regression

While having now reached the end of our search for DMR's, we decided to do a further Logistic regression. This could hopefull show us which sequences amongst our candidates are the most cancer related and important in differentiating phenotypes. In order to perform a Logistic regression with the greatest accuracy possible, we pool our finale data sets. In this manner, the Logistic regression model can draw from all present markers to describe cancer related sequences, rather than only differentiating within genes or promoters. This allows  for the opportunity of a broader scope of the information to be included into the model's calculations.

### Y.1 Creatign a Joint Data Set of Finalists

First we merge each of our final data sets in such a manner that we can still recognize which data set each sequence came from. We then get the Mvalue information for the all the now non-redundant Finalsits and put it all together in a data set along with the assosciated fold change and signficance of each sequence.

```{r Logistic Regression Data Sets}

## Getting M-value data that corresponds to our non-redundant finalists
lg_Mvalues <- g_Mvalues[which(rownames(g_Mvalues) %in% rownames(g_Finale)), ]
lp_Mvalues <- p_Mvalues[rownames(p_Mvalues) %in% rownames(p_Finale), ]

## Formatting and combining our data sets
lg_Mvalues <- cbind(lg_Mvalues, g_Finale$p_values, 
                    g_Finale$Foldchange_Beta)

lp_Mvalues <- cbind(lp_Mvalues, p_Finale$p_values, p_Finale$Foldchange_Beta)

colnames(lg_Mvalues) <- c("AML1", "AML2", "AML3", "AML4", "AML5", "AML6", "AML7", "AML8",
                          "AML9", "AML10","Mon1", "Mon2", "Mon3", "Mon4", "Mon5", "Mon6",
                          "Mon7", "Mon8", "Mon9", "Mon10", "p_values", "Foldchange_Beta")

colnames(lp_Mvalues) <- c("AML1", "AML2", "AML3", "AML4", "AML5", "AML6", "AML7", "AML8",
                          "AML9", "AML10","Mon1", "Mon2", "Mon3", "Mon4", "Mon5", "Mon6",
                          "Mon7", "Mon8", "Mon9", "Mon10", "p_values", "Foldchange_Beta")

row.names(lg_Mvalues) <- paste0(row.names(lg_Mvalues), c("G"))
row.names(lp_Mvalues) <- paste0(row.names(lp_Mvalues), c("P"))

l_Finale_Compound <- rbind(lg_Mvalues, lp_Mvalues)

## Printing the size of new data set
print(paste0("Combined ", nrow(p_Finale), " and ", nrow(g_Finale)," sequences and ended up with a combined size of ",nrow(l_Finale_Compound), " sequences."))

## Previewing our new data set
kable(head(l_Finale_Compound[, c(1:5, 11:15, 21:22)], n = 1), align = "c", digits = 3,)

## Removing uneccesary data sets
remove(lg_Mvalues, g_Mvalues)
remove(lp_Mvalues, p_Mvalues)

```

As we can see in the output, we have eliminated 88 Finalist genes and combined them properly with the promoters finalists. We can also see an excerpt from the new data frame, which has the Mvalue information of each sequence assosciated with it's relevance and significance.

### Y.2 Sorting our Compound Data Set

We will now roughly sort our data set by significance and relevance so that the first vaues to be fed to our Logistic regression model are the most likely to make a significant impact. The fact that we have so many Finalist Sequences that predominantly have positive Fold-Changes and high significances gives an indication that we might have an issue with too many and too highly correlated data, thus we want to give the highest quality sequences to the model first, as these are our most likely research candidates. Since this model is only meant as an indication of how good our predictors are, it is ok for this ordering to be a bit more rough. After all, we are simply asking "do these top finalists really give us the ability to differentiate phennotypes? Are they really cancer assosciated?". 

If we can create an accurate model with them, that question will have been answered in the positive, and we will be creatíng a new ranking anyway with the Logistic regression data.

In the process of this sorting we will also take some formatting steps to make our data set ready for the Logistic regression.


```{r Compound Data Set Ranking and Formatting}
## Ranking our Data Set
  ## Generating a ranking of significance
  l_sig <- rank(l_Finale_Compound$p_values)
  
  ## Generating a ranking for highest absolute value of the fold change
  l_fc_abs <- abs(l_Finale_Compound$Foldchange_Beta)
  l_fc <- rank(-l_fc_abs)
  
  ## Integrating these rankings with one another to generate an overall evaluation of
  ## both relevance and significance
  l_rank <- (l_sig+l_fc)/2
  l_Finale_Compound <- l_Finale_Compound[order(l_rank), 1:20]
  
  ## Creating a preview of how the ranking systen worked
  l_rank_preview <- cbind(l_fc, l_sig, l_rank)
  l_rank_preview <- l_rank_preview[order(l_rank), ]
  colnames(l_rank_preview) <- c("Relevance", "Significance", "Rank")
  
  ## Previewing how the ranking worked
  kable(head(l_rank_preview, n = 4), align = "c" )
  
  ## removing uneccesary datasets
  remove(l_sig, l_fc_abs, l_fc, l_rank_preview)

## Data Set Formatting
  ## We now add a top row for phenotype of the patient and set the AML patients to 1 and
  ## healthy ones to 0
  l_Finale_Compound <- rbind(c(rep(0, 20)), l_Finale_Compound)
  rownames(l_Finale_Compound)[1] <- c("Phenotype")
  l_Finale_Compound[1, ] <- c(rep(1, 10), rep(0, 10))
  
  ## Transposition 
  l_Finale_Compound <- data.frame(t(l_Finale_Compound))
  
## Previewing our new data set
  kable(head(l_Finale_Compound[c(1:5, 11:15), c(1:4, 12:14)], n = 10), align = "c", col.names = c("Phenotype", rep("Sequence", 6)), digits = 3)

```

As we can see from the output, the Compounded data set is now transposed, containing 20 rows of samples and many rows of relevant Sequences. It is also visible that all the samples are marked with a column for phenotype. In cancer the phenotype is "1" and for healthy the phenotype is "0". In this excerpt, some rows have been omitted and the column names have been generalized.

### Y.3 Calculation and Assessment of Regression Models

In this stage we will perform 2 steps of an analysis. First we will calculate a specific Logistic regression model. We will assess it based on its number of regression coeffcients, predictive quality, a few qualities presented in the summary and lastly a type 1 anova.

In the second step, in order to find out if the qualities of the distinct model are comparable to to the qualities of any other model that is based on differing permutations of training and test data sets, we will be measuring predictive capabilties of 20 models based on 20 different permutations of test and training data sets. The reader should be aware that only the predictive quality is being measured for repeated permutations, this is because the main interest of the inquiry was whether or not such a model could be created and predict accurately. 

The other qualities are not of significant interest, because we are not looking to create an actual model, as an actual model has no real application in our line of inquiry.

#### Y.3.1 Single Model Analysis


```{r Regression Coefficients}

## Generating a logitical regression model for further analysis
  ## Sampling random rows from our Compound data set, where 70% of a cohort is always in
  ## training
  set.seed(123)
  
  l_trainseq <- c(sample(seq(1, 10, 1), 7), sample(seq(11, 20, 1), 7))
  l_testseq <- seq(1,20,1)
  l_testseq <- l_testseq[!(l_testseq %in% l_trainseq)]
  
  l_Finale_Compound_train <- l_Finale_Compound[l_trainseq, ]
  l_Finale_Compound_test <- l_Finale_Compound[l_testseq, ]
    
  ## Generating the model
  l_Model <- glm(Phenotype ~., family = binomial(link = 'logit'), data = l_Finale_Compound_test)

## Previewing the number of regression coefficients in the Model
  l_Model_coeff <- summary(l_Model)$coefficients
  kable(l_Model_coeff, align = "c")
  
## Removing unecessary data sets
  remove(l_Finale_Compound_train, l_Finale_Compound_test, l_Model_coeff, l_Model, l_testseq, l_trainseq)
  
```


This output shows us that only five regression coefficients ended up being included in the model. The rest of the sequences were deemed non relevant for the futher analysis. These missing predictors were cut away due to high correlation to the other predictors. In order to keep calculation times efficient we will perform this process again and simply shorten the data set to the relevant variables this time around. It is clear that cutting away these additional sequences cannot impact the model.

```{r Reduced Model}

## Reducing the Compound data set
l_Finale_Compound_red <- l_Finale_Compound[, 1:6]

## Generating the Modle again
set.seed(123)

l_trainseq <- c(sample(seq(1, 10, 1), 7), sample(seq(11, 20, 1), 7))
l_testseq <- seq(1,20,1)
l_testseq <- l_testseq[!(l_testseq %in% l_trainseq)]

l_Finale_Compound_train <- l_Finale_Compound_red[l_trainseq, ]
l_Finale_Compound_test <- l_Finale_Compound_red[l_testseq, ]
  
## Generating the model
l_Model <- glm(Phenotype ~., family = binomial(link = 'logit'), data = l_Finale_Compound_test)

## Analysing the predictive qualities of the model
l_results <- predict(l_Model,newdata=l_Finale_Compound_test,type='response')
l_results <- ifelse(l_results > 0.5, 1, 0)
l_accuracy <- mean(l_results == c(1, 1, 1, 0, 0, 0))
print(paste0("The model assigned the test samples to cohorts with an overall ",100*l_accuracy, " % accuracy."))

## Analysing the summary qualities of the model
summary(l_Model)

## Analysing model quality via type 1 anove
anova(l_Model)

  
## Removing unecessary data sets
remove(l_Finale_Compound_red,l_Finale_Compound_train, l_Finale_Compound_test, l_Model, l_testseq, l_trainseq, l_results, l_accuracy)

```

These three outputs tell us a number of things. For one, it seems that the predictive accuracy of the model is perfect. This indicates that the five sequences included do answer our original queston in the positive.

In addition, we can see from the summary that our residuals with the full model are nigh on 0, again this answers our original question in the positive. In addition we can see that many iterations were required for the model to converge, something that due to the structure of our data indicates separation in our data set. While this is bad if we are trying to fit our model, it is very good when it comes to answering our original question. This falls in line wth the very large regression coefficients and very large standard deviations for these observed, as this is common when models have trouble converging. It also indicates that if we were interested in creating a model we would need a higher n to our sample sizes.

Lastly the anova gives us two points of information. For one, the warnings show us that again there is separation in our data. in addition, the anova shows us that sequence 1 already gives us 0 deviance. Not only is there separation present, it is already visible with just one sequence in the model. 

Given these results it is safe to say that this single Model has fallen victim a very high degree of separation and correlation within our data. This indicates that our first five Finalist sequences are strongly cancer assosciated in our data set and can set the basis of differentiation between the two cohorts.

However, to ensure that this trend holds for the rest of the data and isn't just a byproduct of the starting conditions of the test and training data sets, we will now expand our view.

#### Y.3.2 Multi-Model Analysis

We now analyse the predictive qualities of a multitude of models. We will shorten the compounded data set to always only be the phenotype and one sequence and test the predictive quality of a model relying only on that sequence. We call these types of models "single sequence models". We hope to analyse wether or not single sequence models for all of our Finalist sequences show the same high predictive accuracy due to separation.

In addition, to avoid bias in the starting conditions (which samples are test and which are training) we will run each single sequence model for 100 different variations of the starting conditions.

The warnings R produces have been supressed in this code because they overwhelm the console and document otherwise. While normally they are useful when building a model, they flood our document in our scenario because so many sequences show high separation.

```{r Single Sequence Models}

## Defining a function that creates the test and training data sets and calculates the 
## accuracy of the resulting model
## What is important to note is that the only changing input when this function is used
## is the seed, which is run through 100 different starting conditions

fl_test <- function(x) {
  set.seed(x)
  l_trainseq <- c(sample(seq(1, 10, 1), 7), sample(seq(11, 20, 1), 7))
  l_testseq <- seq(1,20,1)
  l_testseq <- l_testseq[!(l_testseq %in% l_trainseq)]
  
  l_Finale_Compound_train <- l_Finale_Compound_red[l_trainseq, ]
  l_Finale_Compound_test <- l_Finale_Compound_red[l_testseq, ]
  
  l_Model <- glm(Phenotype ~., family = binomial(link = 'logit'),data = l_Finale_Compound_test)
  
  l_results <- predict(l_Model,newdata=l_Finale_Compound_test,type='response')
  l_results <- ifelse(l_results > 0.5, 1, 0)
  l_accuracy <- mean(l_results == c(1, 1, 1, 0, 0, 0))
  return(l_accuracy)
}

## This for loop will test the predictive accuracy for each sequence for 100 random 
## starting conditions (NB: this calculation may take a while)
l_sequence_accuracy <- c()

suppressWarnings(
  for(i in 2:ncol(l_Finale_Compound)) {

    if(i %% floor(ncol(l_Finale_Compound)/100) == 0) {
      cat("|")
    }
    
    l_Finale_Compound_red <- l_Finale_Compound[, c(1,i)]
    
    j <- data.frame(seq(1, 100, 1))
    l_accuracy <- apply(j, 1, fl_test)
    
    l_sequence_accuracy <- c(l_sequence_accuracy, mean(l_accuracy))
    
  }
)
## Previewing what the accuracy looks like overall

plot(ecdf(l_sequence_accuracy)(seq(0,1, 0.001))*100,seq(0,1, 0.001)*100,type = "l", main = "Overview of Single Sequence Model Accuracy", ylab = "Predictive Accuracy [%]", xlab = "% of Models")

abline(h = 80, col = "blue")
abline(v = ecdf(l_sequence_accuracy)(0.8)*100, col = "blue")
abline(v = ecdf(l_sequence_accuracy)(0.9999999999999999)*100, col = "blue")

## Removing uneccesary data sets
remove(i,j, fl_test, l_accuracy, l_Finale_Compound_red)

```

The graph we see here indicates on the y axis a level of accuracy and on the x axis the percentage of single sequence models that are less than or equally accurate to the given level of accuracy. If a line at 50% accuracy intersects the curve at an x value of 0%, then this indicates that 0% of models are less accurate than 50%, and 100% of models are more accurate.

This output shows us that in general our single sequence models perform very well. The incredibly high predictive accuracies show us that most of our sequences show a high degree of separation between the samples. For example, more than 20% of our genes show 100% accuracy over 100 iterations, and over 90% of our values show accuracy above 80%. While this shows that many of our best sequences are similarly accurate, we can still use this as a ranking method for our Finalists.

Most importantly, it is evident that the Logistic regression shows that even with a hand tied behind their backs the vast majority of our finalist sequences can be said to be strongly cancer assosciated and can be used as predictive indicators for phenotype within our data set.

### Y.4 Final Output

We will now rank our finalists and produce the final output of this analysis.

```{r Ranking our Finalsits}

## Creating the l_Order data set which will order each Sequence by it's corresponding
## Single Sequence model's predictive accuracy

l_rank <- sort(l_rank)
l_Order <- data.frame(cbind(l_sequence_accuracy, l_rank))
rownames(l_Order) <- colnames(l_Finale_Compound[, 2:ncol(l_Finale_Compound)])
colnames(l_Order) <- c("Accuracy", "Rank")
l_Order <- l_Order[order(l_Order$Accuracy, decreasing = TRUE), ]

## Sorting the sequences with predictive Accuracy == 1 by their earlier rank
l_Order_Rank_num <- length(which(l_Order$Accuracy == 1))
l_Order_Rank <- l_Order[1:l_Order_Rank_num, ]
l_Order_Rank <- l_Order_Rank[order(l_Order_Rank$Rank), ]
l_Order[1:l_Order_Rank_num, ] <- l_Order_Rank

## Saving the rank and corresponding ensembl ID for each sequence
l_Order <- cbind(rownames(l_Order), seq(1, nrow(l_Order), 1))
l_Order <- data.frame(l_Order)
rownames(l_Order) <- l_Order[, 1]

## Creating a compounded data set from our finalist data sets
row.names(g_Finale) <- paste0(row.names(g_Finale), c("G"))
row.names(p_Finale) <- paste0(row.names(p_Finale), c("P"))

F_comp <- rbind(g_Finale, p_Finale)
F_comp <- F_comp[, c(1,3,4,5)]

# ## Transposing F_comp twice to put dots in its rownames. For some reason transposing our data sets does this and since l_Finale_Compound was transposed (and l_order has identical rownames) we need to transpose F_comp to for the dots to match in both cases
# F_comp <- t(F_comp)
# F_comp <- t(F_comp)

## Merging the compound with the ranks by ensemble IDs
F_comp <- merge(F_comp, l_Order, by = "row.names", all = TRUE)

## Formatting
rownames(F_comp) <- F_comp$Row.names
F_comp <- F_comp[, c(2,3,4,5,7)]
F_comp$X2 <- as.numeric(F_comp$X2)

## Ordering the sequences by their ranks
F_comp <- F_comp[order(F_comp$X2), ]
F_comp <- F_comp[, c(4,3,1,2)]

## Defining our final output
DMRs_AML_Mono <- F_comp

## Previewing our Output
kable(head(DMRs_AML_Mono, n = 10), align = "c")

## Removing uneccesary data sets
remove(l_Order, F_comp, l_Order_Rank, l_Order_Rank_num, l_rank, l_sequence_accuracy, l_Finale_Compound, g_Finale, p_Finale)
```

This final output gives us our "top ten" candidate DMRs produced by the analysis, but obviously goes on to include all 1665 finalist sequences. 




