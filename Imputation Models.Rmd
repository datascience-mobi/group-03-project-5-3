---
title: "Imputation Models"
author: "Leo Burmedi, Pierre de Marinis, Konstantin Fischer, Daniel Ãœrge"
date: "7/10/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# A. Imputation Models

Rather than simply impute the mean for our NAs we decided to do a more in depth analysis of the effects of varying imputation models on our data. Specifically, we were interested in how the imputation would impact the T-tests we are going to conduct further down the road. We also conclude with a look at which imputation models hold true to the original data structure the best.

In the first part of our analysis, we study the false positive rate by applying our imputation models in test scenarios where all tests are false, and looking at what the histogramms of p-values look like. To clarify, we take two samples of the same distribution, "lose" three values and impute them. We then perform a significance test. As the original distributions were identical, in all cases the p-value distributionn should be equally distributed. We look at the histogramms for the different methods and infer the deviance from this normal distribution for every method.

This is the metric we use to determine which imputation model produces the most realistic rate of false positives. Because of multiple testing correction later, it is especially important to us that the false positive rate remain quantifyable, and therefore, realsitic.

The direct relevance of this model to our data set is that it replicates a scenario in which imputation has the largest influence on our data set. Since our data set allows a maximum of 3 NAs per cohort, the highest degree of imputation would be 3 values in two cohorts that are compared to each other. Since we care about the false positive rate the most, we also model a situation in which a false positive can occur.

### A.1 Perfect Imputation

As a yardstick to measure all the other methods by, we will start by defining the perfect case scenario. 

We take two random samples from an identical standard normal distribution. Then we would lose 3 values and need to impute them. In the perfect scenario the new values would come from the same original standard normal distribution. Therefore we simply do not remove the three NAs in the first place.

```{r Perfect Imputation}
## Creating a data frame that will serve as the set.seed imputs and dictate the number of iterations the model goes through
j <- data.frame(c(seq(1,500000, 1)))
## Creating jcount, a variable that will help us mark whenever 1% of a task has been done (later in the progress bars)
jcount <- floor(nrow(j)/100)

## We define a function that models what a perfect imputation would look like.
f_model_perfect_imputation <- function(x) {

  ## We set up a progress bar for ease of the user
  if(x %% jcount == 0) {
    cat("|")
  }
  
  ## We set the seed for reproduceable results (note that we set a different seed for each sample!), and then draw our random samples
  set.seed(x)
  sample1 <- c(rnorm(10))
  set.seed(-x)
  sample2 <- c(rnorm(10))
  
  ## We "lose and reimpute perfectly" by doing nothiing
  
  ## We perform a significance test to determine whether our samples originate from the same distribution
  t_testresults <- t.test(sample1, sample2, var.equal = TRUE)
  
  ## We return the p-value
  return(c(t_testresults["p.value"]))
}

## Here we apply our new function across j and format our results
perfectimputation <- apply(j, 1, f_model_perfect_imputation)
perfectimputation <- unlist(perfectimputation)

## A preview of how th false positive rate might be skewed
print(paste0("In perfect imputation ", round(ecdf(perfectimputation)(0.05)*100, digits = 2), " % of p-values lie under the 5% mark. An alpha of 5% causes ", round(ecdf(perfectimputation)(0.05)/0.05, digits = 2), " times more false positives than would be expected."))

```

At this point we will not plot the results yet, as they will be more illuminating when they are all simultaneously compared. However, this output shows us that the perfect imputation is nearing the proper equal distribution upon repeated iterations of  the model.

### A.2 Mean Imputation

The classic method for imputing values is to insert the mean of the present values for the NAs. To model this, we will be taking two samples from identical distributions, "losing" three values from them and impute them with the mean. We will theen perform a t-test to try and determine wether or not these two samples are significantly different from one another. Once this process is executed 500.000 times we will see if the distribution of p-values differs from the perfect imputation.

```{r Mean Imputation}
## We define a function that models what a mean imputation would look like.
    f_model_mean_imputation <- function(x) {
      
      ## We set up a progress bar for ease of the user
      if(x %% jcount == 0) {
        cat("|")
      }
      
      ## We set the seed for reproduceabble results
      set.seed(x)
      
      ## We take a sampe of seven
      sample1 <- c(rnorm(7))
      
      ## We impute three more values to make up for the ones we "lost"
      sample_impute <- c(rep(mean(sample1), 3))
      sample1 <- c(sample1, sample_impute)
      
      ## Same process as above
      set.seed(-x)
      sample2 <- c(rnorm(7))
      sample_impute <- c(rep(mean(sample2), 3))
      sample2 <- c(sample2, sample_impute)
      
      ## We apply a t-test for significance and return the p-value
      t_testresults <- t.test(sample1, sample2, var.equal = TRUE)
      return(c(t_testresults["p.value"]))
    }

## Here we apply our new function across j and format our results
meanimputation <- apply(j, 1, f_model_mean_imputation)
meanimputation <- unlist(meanimputation)

## A preview of how th false positive rate might be skewed
print(paste0("In mean imputation ", round(ecdf(meanimputation)(0.05)*100, digits = 2) , " % of p-values lie under the 5% mark. An alpha of 5% causes ", round(ecdf(meanimputation)(0.05)/0.05, digits = 2), " times more false positives than would be expected."))
```

This output shows us that mean imputation causes a great deal more false positives than a normal equal distribution, something we will see in greater detail later.

### A.3 Statistical Imputation

In this approach that is also common we will assume that the M-distribution is close enough to the normal distribution for the model using the normal distribution as samples to produce the same effects. This assumption, that the m distribution and normal distribution are identical for our purposes, is a repeated assumption in our analysis. 

Since we know that the original distribution of our M-values is an M-dsitribution we could attempt to impute NAs statistically, that is to take random values from a normal distribution with the same mean and standard deviation as the sample. In this manner we mimic what the original distribution looked like by estimating its qualities based on the sample. This process is referred to as Rnorm Imputation because it makes use of the function rnorm.

```{r Rnorm Imputation}

## We define a function that models what an rnorm imputation would look like.
  f_model_rnorm_imputation <- function(x) {
    
    ## We set up a progress bar for ease of the user
    if(x %% jcount == 0) {
      cat("|")
    }
    
    ## We set the seed for reproduceabble results
    set.seed(x)
    
    ## We take a sampe of seven
    sample1 <- c(rnorm(7))
    
    ## We set a different seed for the imputation
    set.seed(x + (nrow(j)))
    
    ## We impute three more values by mimicing the original distribution
    sample_impute <- c(rnorm(mean = mean(sample1), sd = sd(sample1), 3))
    sample1 <- c(sample1, sample_impute)
    
    ## We repeat the same process as above
    set.seed(-x)
    sample2 <- c(rnorm(7))
    set.seed(-x - (nrow(j)))
    sample_impute <- c(rnorm(mean = mean(sample2), sd = sd(sample2), 3))
    sample2 <- c(sample2, sample_impute)
    
    ## We apply a t-test for significance and return the p-value
    t_testresults <- t.test(sample1, sample2, var.equal = TRUE)
    return(c(t_testresults["p.value"]))  
    
  }


## Here we apply our new function across j and format our results
rnormimputation <- apply(j, 1, f_model_rnorm_imputation)
rnormimputation <- unlist(rnormimputation)

## A preview of how th false positive rate might be skewed
print(paste0("In rnorm imputation ", round(ecdf(rnormimputation)(0.05)*100, digits = 2), " % of p-values lie under the 5% mark. An alpha of 5% causes ", round(ecdf(rnormimputation)(0.05)/0.05, digits = 2), " times more false positives than would be expected."))

```

A similar output as mean impuatation.

### A.4 Rnorm10 Imputation

We hypothesized that imputing the mean into a data set would increase its rate of significant results because it decreases the standard deviation of the sample. This inspiration lead us to extrapolate that perhaps the shifted rate of false positives derives in the altered mean and standard deviation before and after imputation. In search for an improved method of imputation, we designed "Rnorm10  Imputation", which repeats the Rnorm Imputation process 10 times generating 10 sets of 3 values that ought to be imputed. It then selects for the set of the 10 that alters the mean and standard deviation of the sample as little as possible, whereby mean and standard deviation are weighted equally.

```{r Rnorm10 Imputation}

## We define a function that models what an rnorm10 imputation would look like.
f_model_rnorm10_imputation <- function(x) {
  
  ## We set up a progress bar for ease of the user
  if(x %% jcount == 0) {
    cat("|")
  }
  
  ## We set the seed to get reproduceable results
  set.seed(x)
  
  ## We take our first sample of seven values
  sample1 <- c(rnorm(7))
  
  ## We determine what the mean and standard deviation are before imputation
  pre_imputation_stats <- c(mean(sample1), sd(sample1))
  
  ## We define the data set that our 10 sets of 3 values will be going into
  winnercandidates <- c()
  
  ## We start the loop that will perform rnorm imputation 10 times
  for (i in 1:10) {
    
    ## Set seed for reproducibility
    set.seed(x + (i * nrow(j)))
    
    ## rnorm imputation
    sample_impute <- c(rnorm(mean = mean(sample1), sd = sd(sample1), 3))
    
    ## we create a vector of our sample and the imputed values
    sample1_post_imputation <- c(sample1, sample_impute)
    
    ## We collect the new mean and standard deviation post imputation
    post_imputation_stats <- c(mean(sample1_post_imputation), sd(sample1_post_imputation))
    
    ## We determine how much the mean and sd have changed, and quantify this
    impdiff <- (post_imputation_stats - pre_imputation_stats)
    impdiff <- (sum(abs(impdiff)))
    
    ## We save our set of three values alongside the impdiff corresponding to them
    candidate <- c(impdiff, sample_impute)
    winnercandidates <- cbind(winnercandidates, candidate)
    
    
  }
  
  ## We format the winnercandidates data frame
  winnercandidates <- data.frame(winnercandidates)
  
  ## We determine which set of values has the lowest impdiff
  winnercol <- which.min(winnercandidates[1,])
  
  ## We determine which set of three values corresponds to the lowest impdiff
  winner <-winnercandidates[2:nrow(winnercandidates), winnercol]
  
  ## Sample1 is imputed with the values corresponding to the lowest impdiff
  sample1 <- c(sample1, winner)
  
  ## We repeat the identical process for sample2
  set.seed(-x)
  sample2 <- c(rnorm(7))
  preextendstats <- c(mean(sample2), sd(sample2))
  
  winnercandidates <- c(rep(0, 4))
  
  for (i in 1:10) {
    set.seed(-x - (i * nrow(j)))
    sample_impute <-
      c(rnorm(mean = mean(sample2), sd = sd(sample2), 3))
    sample2a <- c(sample2, sample_impute)
    
    postextendstats <- c(mean(sample2a), sd(sample2a))
    impdiff <- (postextendstats - preextendstats)
    impdiff <- (sum(abs(impdiff)))
    
    candidate <- c(impdiff, sample_impute)
    winnercandidates <- cbind(winnercandidates, candidate)
    
    
  }
  
  winnercandidates <- data.frame(winnercandidates)
  winnercandidates <-
    winnercandidates[, 2:ncol(winnercandidates)]
  winnercol <- which.min(winnercandidates[1,])
  winner <-
    winnercandidates[2:nrow(winnercandidates), winnercol]
  
  sample2 <- c(sample2, winner)
  
  ##  We apply a t-test to determine whether sample1 and sample2 are significantly different
  t_testresults <- t.test(sample1, sample2, var.equal = TRUE)
  
  ## We return the p-value
  return(c(t_testresults["p.value"]))
} 


## Here we apply our new function across j and format our results
rnorm10imputation <- apply(j, 1, f_model_rnorm10_imputation )
rnorm10imputation <- unlist(rnorm10imputation)

## A preview of how th false positive rate might be skewed
print(paste0("In rnorm10 imputation ", round(ecdf(rnorm10imputation)(0.05)*100, digits = 2), " % of p-values lie under the 5% mark. An alpha of 5% causes ", round(ecdf(rnorm10imputation)(0.05)/0.05, digits = 2), " times more false positives than would be expected."))

```

A similar output as rnorm imputation.

### A.5 Summarizing and Visualizing our Models

Here we will visualize and summarize the effect of the different imputation types.

```{r Visualization and Summary}

## We calculate the mode of the mean imputation histogramm and the average value of the perfect imputation histogramm, these well help us keep track of the worst and best case scenario in every histogramm
  ## We start by creating histogramm objects and working with their count information
  mean_imputation_hist     <- hist(meanimputation, 40, plot = FALSE)
  perfect_imputation_hist  <- hist(perfectimputation, 40, plot = FALSE)
  
  ## We take the mean of al the counts for the value an equal distribution would produce at every bin
  perfectaverage <- mean(perfect_imputation_hist$counts)
  ## We take the mode of the mean distribution because we suspect it to be the highest value in all the plots
  meanmode <- max(mean_imputation_hist$counts)


## We set up the layout of the output
par(mfrow = c(2,2), mar = c(4,4,3,1))

## Our four historgamms for our results
hist(perfectimputation, 40, ylim = c(0, meanmode), col = "#99CCFF", main = "", xlab = "p-values")
title("Perfect Imputation", font.main = 1, line = -1.5)
abline(h = perfectaverage)

hist(meanimputation, 40, ylim = c(0, meanmode), col = "#99CCFF", main = "", xlab = "p-values")
title("Mean Imputation", font.main = 1, line = -1.5)
abline(h = perfectaverage)

hist(rnormimputation, 40, ylim = c(0, meanmode), col = "#99CCFF", main = "", xlab = "p-values")
title("Rnorm Imputation", font.main = 1, line = -1.5)
abline(h = perfectaverage)

hist(rnorm10imputation, 40, ylim = c(0, meanmode), col = "#99CCFF", main = "", xlab = "p-values")
title("Rnorm10 Imputation", font.main = 1, line = -1.5)
abline(h = perfectaverage)

mtext("P-Value Distributions depending on Imputation Method", cex = 1.5, side = 3, line = - 1.5, outer = TRUE)

## Removing uneccesary data
remove(meanmode, perfectaverage, f_model_mean_imputation, f_model_perfect_imputation, f_model_rnorm_imputation, f_model_rnorm10_imputation, perfect_imputation_hist, mean_imputation_hist, perfectimputation, meanimputation, rnormimputation, rnorm10imputation)

```

The output shows us that while our differing methods produce differing results, and rnorm10 does actually perform better than any other (non-perfect) method, all of them deviate drastically from the perfect case scenario (we also modeled imputation using the median and a model fo hot-decking, both of which showed similar results). This makes sense, as a sample size of n can only represent it's original distribution so well. Even if we imputed and did not change the mean or standard deviation at all, we would still be increasing n, i.e. our confidence in the information, without adding any information to the data set. It is intuitive that increasing our confidence without adding the basis for said confidence causes false positives.

In the face of these results we decided that no imputation method would be acceptably accurate for our t-tests, and we decided on a "null model" of not imputing at all. After all, a t-test only needs certain inputs that can be collected from a data set with NAs still in it, so it is not necessary to perform imputation for the t-tests. With no imputaton, the rate of false positives cannot be affected, and our multiple testing corrected p-values will be realistic and accurate. While we trade for power in this scenario, we feel the trade is worth it in our context.

However, it is required for all cells to be assigned a value for the PCA, so we still need to find a solution for imputation there. The above plots seem to indicate that rnorm10 is a good option. To create a comparative spectrum we will analyse a perfect case scenario, the rnorm10 scenario, and the standard scenario, which is mean imputation.

### A.6 Analysing Data Structure for Mean Imputation and rnorm10 Imputation

If a data set is normally distributed, it's entire data structure can be given with its mean and standard deviation. We have already compared how the mean and standard deviation are altered by mean and rnorm imputation by comparing them with t-tests earlier. These results have clearly indicated that mean imputation makes two distributions appear more distinct than rnorm10 imputation does, telling us that given normal distribution, rnorm10 reports results truer to the original than mean imputation.

Having analysed these two factors it is time to answer whether or not the the remaining element of data structure, i.e. the distribution of the data, is affected by Imputation and how. After all it doesn't matter how true to the original the mean and standard deviation are if the data does not actually fit a normal distribution. We analyse the normailty of the distributions by performing a Shapiro-Wilk test on our data to determine if it is significantly non-normally distributed. We will consider p-values below 0.1 significant, as was suggested by ~~Patrick Royston (1995). Remark AS R94: A remark on Algorithm AS 181: The W test for normality. Applied Statistics, 44, 547â€“551. doi: 10.2307/2986146.~~
 
```{r Data Structure Comparison}

## We define a function that will create a sample and test its normality, while also imputing it twice and testing it's normality then.
f_model_data_structure <- function(x) {
  
  ## Progress bar
  if(x %% jcount == 0) {
    cat("|")
  }

  ## We get a reproduceable sample1
  set.seed(x)
  sample1 <- c(rnorm(7))
  
  ## We perform rnorm10 imputation on sample1 to produce an imputed sample called sample_rnorm10
  pre_imputation_stats <- c(mean(sample1), sd(sample1))
  winnercandidates <- c()
  
  for (i in 1:10) {
    set.seed(x + (i * nrow(j)))
    
    sample_impute <- c(rnorm(mean = mean(sample1), sd = sd(sample1), 3))
    sample1_post_imputation <- c(sample1, sample_impute)
    
    post_imputation_stats <- c(mean(sample1_post_imputation), sd(sample1_post_imputation))
    impdiff <- (post_imputation_stats - pre_imputation_stats)
    impdiff <- (sum(abs(impdiff)))
    
    candidate <- c(impdiff, sample_impute)
    winnercandidates <- cbind(winnercandidates, candidate)
    
  }
  
  winnercandidates <- data.frame(winnercandidates)
  winnercol <- which.min(winnercandidates[1,])
  winner <-winnercandidates[2:nrow(winnercandidates), winnercol]
  
  ## We get rnrom10
  sample_rnorm10 <- c(sample1, winner)
  
  ## We perform mean imputation on sample1 to produce an imputed sample called sample_mean
  sample_impute <- c(rep(mean(sample1), 3))
  sample_mean <- c(sample1, sample_impute)
  
  ## We perform our three shapiro-Wilk tests
  data_structure_sample1 <- shapiro.test(sample1)$p.value
  data_structure_rnorm10 <- shapiro.test(sample_rnorm10)$p.value
  data_structure_mean <- shapiro.test(sample_mean)$p.value
  
  ## We report the p-value of our non-imputed sample, our rnorm10 sample and our mean imputed sample
  return(c(data_structure_sample1, data_structure_mean, data_structure_rnorm10))
} 


## Here we apply our new function across j and formatting our resullts
imputation_data_structure <- apply(j, 1, f_model_data_structure)
imputation_data_structure <- t(imputation_data_structure)

## Creating histogramm objects for an in depth comparison
mean_imputation_hist     <- hist(imputation_data_structure[, 2], 30, plot = FALSE)
rnorm10_imputation_hist  <- hist(imputation_data_structure[, 3], 30, plot = FALSE)
no_imputation_hist       <- hist(imputation_data_structure[, 1], 30, plot = FALSE)

## Determining what frequencies an equal distribution would give in a histogramm
perfectaverage <- mean(no_imputation_hist$counts)

## Dtermining the highest value in the data set
data_set_mode <- max(c(mean_imputation_hist$counts, rnorm10_imputation_hist$counts, no_imputation_hist$counts))

## Determining how strongly our three distributions bin counts deviate from the value an equal distribution would have in each bin
mean_deviance <- mean(abs(mean_imputation_hist$counts - perfectaverage))
rnorm10_deviance <- mean(abs(rnorm10_imputation_hist$counts - perfectaverage))
no_imputation_deviance <- mean(abs(no_imputation_hist$counts - perfectaverage))

deviance_from_equal <- c(mean_deviance, rnorm10_deviance, no_imputation_deviance)

## Determining what percentage of of vectors would have been considered non-normally distributed
percent_normal_mean <- ecdf(imputation_data_structure[, 2])(0.1)
percent_normal_rnrom10 <- ecdf(imputation_data_structure[, 3])(0.1)
percent_normal_no_imputation <- ecdf(imputation_data_structure[, 1])(0.1)

percent_non_normal <- c(percent_normal_mean, percent_normal_rnrom10, percent_normal_no_imputation)

## Plotting our P-Value Histogramms
par(mfrow = c(1, 3), mar = c(4, 4, 4, 1))

hist(imputation_data_structure[, 2], 30, ylab = "Frequency", xlab = "p-value", col = "#99CCFF", ylim = c(0, data_set_mode), main = "")
title("Mean Imputation", line = -0.25, font.main = 1)
abline(h = perfectaverage)

hist(imputation_data_structure[, 3], 30,ylab = "Frequency", xlab = "p-value", col = "#99CCFF", ylim = c(0, data_set_mode), main = "")
title("Rnorm10 Imputation", line = -0.25, font.main = 1)
abline(h = perfectaverage)

hist(imputation_data_structure[, 1], 30, ylab = "Frequency", xlab = "p-value", col = "#99CCFF", ylim = c(0, data_set_mode), main = "")
title("No Imputation", line = -0.25, font.main = 1)
abline(h = perfectaverage)

mtext("P-Value Distributions for Normality", cex = 1.5, side = 3, line = - 2, outer = TRUE)

## Plotting our In Depth Results
par(mfrow = c(1, 2), mar = c(4, 4, 5, 1))

barplot(deviance_from_equal, names.arg = c("Mean", "Rnorm10", "Null"), ylab = c("Counts"), font.main = 1, col = c("#CCCCCC", "#B2CCE5", "#99CCFF"))
title("Deviance from Equal Distribution", line = 1, font.main = 1)

barplot(percent_non_normal, names.arg = c("Mean", "Rnorm10", "Null"), ylab = c("Percent"), font.main = 1, col = c("#CCCCCC", "#B2CCE5", "#99CCFF"))
title("% of Distributions Non-Normal", line = 1, font.main = 1)

mtext("In Depth Imputation Effects", cex = 1.5, side = 3, line = - 1.5, outer = TRUE)

## Removing uneccesary Data Sets
remove(data_set_mode, deviance_from_equal, f_model_data_structure, imputation_data_structure, j, jcount, mean_deviance, mean_imputation_hist, no_imputation_hist, no_imputation_deviance, percent_non_normal, percent_normal_mean, percent_normal_no_imputation, percent_normal_rnrom10, perfectaverage, rnorm10_deviance, rnorm10_imputation_hist)

```

As both of these outputs show us, rnorm10 did not perform perfectly but certainly outperformed mean imputation. The p-value distributions may look a little unusual (dip and peak in non-imputation), this likely has to do with the fact that the Shapiro-Wilk test produces estimated p-values instead of exact ones. However, it is still quite apparent that the rnorm10 distribution looks much closer to an equal distribution than the mean distribution.

A more in depth quantitave look at the data shows us the deviance from the equal distribution and the percentage of non-normal distributions present. The firstgraph shows us the mean absolute difference between thee number of counts per bin and the number that would be expected from an equal distribution. As we can see, mean imputation has about three times as much deviance from an equal distribution than Rnorm10. The second graph shows us what percentage of the generated distributions would be considered non-normal with a significance threshold of 0.1%. Rnorm10 shows an almost identical amount of Non-normal models as the nulll model, which both fall right in line with an alpha of 10%. Mean imputation on the other hand shows twice as many models as would be expected to be non-normal.

In summary, if a sample of 7 normally distributed values is imputed with Rnorm10 Imputation, it will have a significantly more realistic mean and standard deviation representing the original distribution, and it will keep the normal distribution of the sample intact. If another of the methods we tested is used instead, these qualities are not a given.